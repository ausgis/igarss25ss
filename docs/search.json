[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IEEE IGARSS 2025 Summer School",
    "section": "",
    "text": "Preface\n\n\n\n\n\nThe conference website link: https://www.2025.ieeeigarss.org/.\nTopic:Tools for remote sensing and geospatial intelligence analysis: An example of climate impact on bushfire.\n\n\n\n\n\n\nNote\n\n\n\nIn each section, we will include Tools, Aim, Description of steps, and Code.\nPlease ensure that RStudio is installed on your computer and that you have signed up for a Google Earth Engine account. Below are the instructions for installing R and RStudio, as well as signing up for Google Earth Engine. Kindly follow the steps provided in the links:\n\nGuide for Installing R and RStudio: https://rstudio-education.github.io/hopr/starting.html\nGuide for Signing Up for Google Earth Engine: https://courses.spatialthoughts.com/gee-sign-up.html",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "sec1.html",
    "href": "sec1.html",
    "title": "\n1  Remote sensing data collection\n",
    "section": "",
    "text": "1.1 Define study area\nFigure 1.1: The location of study area, West Daly, in Australia",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Remote sensing data collection</span>"
    ]
  },
  {
    "objectID": "sec1.html#collecting-bushfire-data",
    "href": "sec1.html#collecting-bushfire-data",
    "title": "\n1  Remote sensing data collection\n",
    "section": "\n1.2 Collecting bushfire data",
    "text": "1.2 Collecting bushfire data\n\n\n\n\n\n\nAim\n\n\n\nThis code is designed to compute and export the annual burned area from the MODIS MCD64A1 dataset using Google Earth Engine (GEE).\n\n\n\n\n\n\n\n\nDescription of steps\n\n\n\n\nLoad the MODIS MCD64A1 burned area dataset from GEE.\nDefine a function to clip and export burned area data by year.\nSet a time range (January 1st to December 31st of the given year).\nFilter the dataset for the selected year and extract the burned area information.\nApply an aggregation method (e.g., mean) to summarize burned area data.\nClip the data to the Region of Interest (ROI).\nExport the processed burned area data to Google Drive as a GeoTIFF.\nLoop through the desired years (2000-2024) and execute the function.\n\n\n\n// Load the MODIS MCD64A1 dataset\nvar dataset = ee.ImageCollection(\"MODIS/061/MCD64A1\");\n\n// Define a function to clip the dataset and export it by year\nfunction exportYearlyBurnedArea(year) {\n  // Create a date range for the specific year\n  var startDate = ee.Date.fromYMD(year, 1, 1);\n  var endDate = startDate.advance(1, 'year');\n\n  // Filter the dataset for the specific year and clip to the ROI\n  var yearlyBurnedArea = dataset.filterDate(startDate, endDate)\n                                .select('BurnDate')\n                                .mean()  // Or use another appropriate aggregation method\n                                .clip(roi.geometry().bounds());\n\n  // Export the processed data\n  Export.image.toDrive({\n    image: yearlyBurnedArea,\n    description: 'BurnedArea_' + year,\n    scale: 500,  // Adjust resolution as needed\n    region: roi,\n    fileFormat: 'GeoTIFF'\n  });\n}\n\n// Loop through and export data for the years 2000 to 2024\nfor (var year = 2000; year &lt;= 2024; year++) {\n  exportYearlyBurnedArea(year);\n}\nThe GEE code link: https://code.earthengine.google.com/c4c9731308de7f49c6e468c3daa8cb03 .",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Remote sensing data collection</span>"
    ]
  },
  {
    "objectID": "sec1.html#collecting-climate-data",
    "href": "sec1.html#collecting-climate-data",
    "title": "\n1  Remote sensing data collection\n",
    "section": "\n1.3 Collecting climate data",
    "text": "1.3 Collecting climate data\n\nTemperature\n\n\n\n\n\n\n\nAim\n\n\n\nThis code is designed to compute and export the annual mean temperature from the ERA5-Land Hourly Temperature dataset using GEE.\n\n\n\n\n\n\n\n\nDescription of steps\n\n\n\n\nLoad ERA5 hourly temperature data from Google Earth Engine.\nDefine a function to compute the annual mean temperature.\nSet a time range (January 1st to December 31st of the given year).\nFilter the dataset for the given year and compute the mean temperature.\nConvert temperature from Kelvin to Celsius.\nClip the data to the ROI.\nExport the processed temperature data to Google Drive as a GeoTIFF.\nLoop through the desired years and execute the function.\n\n\n\n// Load the ERA5 daily temperature dataset\nvar dataset = ee.ImageCollection(\"ECMWF/ERA5_LAND/HOURLY\");\n\n// Define a function to calculate and export the annual mean temperature\nfunction exportYearlyTemperature(year) {\n  // Create the date range\n  var startDate = ee.Date.fromYMD(year, 1, 1);\n  var endDate = startDate.advance(1, 'year');\n\n  // Filter the dataset and compute the annual mean temperature (unit: K)\n  var yearlyTemperature = dataset.filterDate(startDate, endDate)\n                                 .select('temperature_2m')\n                                 .mean()  // Compute annual mean temperature\n                                 .subtract(273.15)  // Convert to Celsius\n                                 .clip(roi.geometry().bounds());\n\n  // Export the result to Google Drive\n  Export.image.toDrive({\n    image: yearlyTemperature,\n    description: 'Tem' + year,\n    scale: 5000,  // ERA5 resolution, recommended 5km (5000m)\n    region: roi,\n    fileFormat: 'GeoTIFF'\n  });\n}\n\n// Loop to calculate the annual mean temperature for the years 2000-2024\nfor (var year = 2000; year &lt;= 2024; year++) {\n  exportYearlyTemperature(year);\n}\nThe GEE code link: https://code.earthengine.google.com/5a7f743bb1d0bd174f1d199e26dc4d61 .\n\nPrecipitation\n\n\n\n\n\n\n\nAim\n\n\n\nThis code is designed to compute and export the annual cumulative precipitation from the CHIRPS 5-day interval precipitation dataset using GEE.\n\n\n\n\n\n\n\n\nDescription of steps\n\n\n\n\nLoad CHIRPS 5-day interval precipitation data from Google Earth Engine.\n\nDefine a function to compute the annual cumulative precipitation.\n\nSet a time range (January 1st to December 31st of the given year).\n\nFilter the dataset for the given year and compute the total precipitation.\n\nClip the data** to the ROI.\n\nExport the processed precipitation data to Google Drive as a GeoTIFF.\n\nLoop through the desired years (2000-2024) and execute the function.\n\n\n\n// Load the CHIRPS dataset (5-day interval precipitation)\nvar dataset = ee.ImageCollection(\"UCSB-CHG/CHIRPS/PENTAD\");\n\n// Define a function to calculate and export the annual cumulative precipitation\nfunction exportYearlyPrecipitation(year) {\n  // Create the date range\n  var startDate = ee.Date.fromYMD(year, 1, 1);\n  var endDate = startDate.advance(1, 'year');\n\n  // Filter the dataset and compute the total precipitation for the year\n  var yearlyPrecipitation = dataset.filterDate(startDate, endDate)\n                                   .select('precipitation')\n                                   .sum()  // Compute annual total precipitation\n                                   .clip(roi.geometry().bounds());\n\n  // Export the result to Google Drive\n  Export.image.toDrive({\n    image: yearlyPrecipitation,\n    description: 'Pre' + year,\n    scale: 5000,  // CHIRPS resolution (~5.5 km), adjustable\n    region: roi,\n    fileFormat: 'GeoTIFF'\n  });\n}\n\n// Loop to compute annual cumulative precipitation for the years 2000-2024\nfor (var year = 2000; year &lt;= 2024; year++) {\n  exportYearlyPrecipitation(year);\n}\nThe GEE code link: https://code.earthengine.google.com/e8b09a65c7d506243e0895d7b6af4e41 .",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Remote sensing data collection</span>"
    ]
  },
  {
    "objectID": "sec2.html",
    "href": "sec2.html",
    "title": "2  Temporal analysis for remote sensing data",
    "section": "",
    "text": "2.1 Temporal trend analysis for bushfire\nCode (R version):\nCode (Python version):",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Temporal analysis for remote sensing data</span>"
    ]
  },
  {
    "objectID": "sec2.html#temporal-correlation-analysis-between-bushfire-and-climate",
    "href": "sec2.html#temporal-correlation-analysis-between-bushfire-and-climate",
    "title": "2  Temporal analysis for remote sensing data",
    "section": "2.2 Temporal correlation analysis between bushfire and climate",
    "text": "2.2 Temporal correlation analysis between bushfire and climate\n\n\n\n\n\n\nAim\n\n\n\nThis code is designed to compute the Spearman correlation coefficients and p-values between burned area and precipitation/temperature for each grid cell over the years 2000-2024, and save the results as a new shapefile.\n\n\n\n\n\n\n\n\nDescription of steps\n\n\n\n\nLoad necessary libraries (sf for spatial data and dplyr for data manipulation).\nRead the shapefile containing burned area, precipitation, and temperature data.\nExtract column names corresponding to:\nBurned area (BA_2000 to BA_2024)\nPrecipitation (Pre2000 to Pre2024)\nTemperature (Tem2000 to Tem2024)\nCompute the Spearman correlation for each grid cell:\nBurned area vs. Precipitation (cor_BA_Pre)\nBurned area vs. Temperature (cor_BA_Tem)\nExtract the correlation coefficient (estimate) and p-value (p.value) for statistical significance.\nSave the updated shapefile containing correlation results.\n\n\n\nCode (R version):\n# Load necessary libraries\nlibrary(sf)\nlibrary(dplyr)\n\n# Read the shapefile\ngrid &lt;- st_read(\"/your_path_here/grid5_Sta.shp\")\n\n# Extract column names (ensure column order matches years)\nba_cols &lt;- paste0(\"BA_\", 2000:2024)   # Burned area columns\npre_cols &lt;- paste0(\"Pre\", 2000:2024)  # Precipitation columns\ntem_cols &lt;- paste0(\"Tem\", 2000:2024)  # Temperature columns\n\n# Compute the Spearman correlation and p-values for each grid cell\ngrid &lt;- grid %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    cor_BA_Pre = cor.test(as.numeric(c_across(all_of(ba_cols))), \n                          as.numeric(c_across(all_of(pre_cols))), \n                          method = \"spearman\", use = \"complete.obs\")$estimate,\n    p_BA_Pre = cor.test(as.numeric(c_across(all_of(ba_cols))), \n                        as.numeric(c_across(all_of(pre_cols))), \n                        method = \"spearman\", use = \"complete.obs\")$p.value,\n    \n    cor_BA_Tem = cor.test(as.numeric(c_across(all_of(ba_cols))), \n                          as.numeric(c_across(all_of(tem_cols))), \n                          method = \"spearman\", use = \"complete.obs\")$estimate,\n    p_BA_Tem = cor.test(as.numeric(c_across(all_of(ba_cols))), \n                        as.numeric(c_across(all_of(tem_cols))), \n                        method = \"spearman\", use = \"complete.obs\")$p.value\n  ) %&gt;%\n  ungroup()\n\n# Save the new shapefile\nst_write(grid, \"/your_path_here/grid5_Cor.shp\", delete_layer = TRUE)\nCode (Python version):\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import pearsonr\n\n# Read the Shapefile\ngdf = gpd.read_file(\"/your_path_here/grid5_Sta_slope.shp\")\n\n# Extract column names for the time series\nyears = list(range(2000, 2025))  # 2000 to 2024\nba_columns = [f\"BA_{year}\" for year in years]\ntem_columns = [f\"Tem{year}\" for year in years]\npre_columns = [f\"Pre{year}\" for year in years]\n\n# Ensure all relevant columns exist in the dataset\nba_columns = [col for col in ba_columns if col in gdf.columns]\ntem_columns = [col for col in tem_columns if col in gdf.columns]\npre_columns = [col for col in pre_columns if col in gdf.columns]\n\n# Check if any required columns are missing\nif not ba_columns or not tem_columns or not pre_columns:\n    raise ValueError(\"BA, Tem, or Pre-related columns are missing. Please check the data file.\")\n\nprint(\"BA Columns:\", ba_columns)\nprint(\"Tem Columns:\", tem_columns)\nprint(\"Pre Columns:\", pre_columns)\n\n# Ensure all data columns are numeric\nfor col in ba_columns + tem_columns + pre_columns:\n    gdf[col] = pd.to_numeric(gdf[col], errors='coerce')\n\n# Function to compute Pearson correlation (excluding zero values)\ndef compute_correlation(row, x_columns):\n    ba_values = row[ba_columns].values.astype(float)\n    x_values = row[x_columns].values.astype(float)\n    \n    # Check if arrays are empty\n    if len(ba_values) == 0 or len(x_values) == 0:\n        return np.nan, np.nan\n    \n    # Filter out NaN and zero values\n    mask = (ba_values != 0) & (x_values != 0) & ~np.isnan(ba_values) & ~np.isnan(x_values)\n    ba_filtered = ba_values[mask]\n    x_filtered = x_values[mask]\n    \n    # Ensure both arrays have at least two valid values\n    if len(ba_filtered) &lt; 2 or len(x_filtered) &lt; 2:\n        return np.nan, np.nan\n    \n    correlation, p_value = pearsonr(ba_filtered, x_filtered)\n    return correlation, p_value\n\n# Compute Pearson correlation and significance level between BA and Tem\ngdf[[\"BA_Tem_corr\", \"BA_Tem_pval\"]] = gdf.apply(lambda row: compute_correlation(row, tem_columns), axis=1, result_type='expand')\n\n# Compute Pearson correlation and significance level between BA and Pre\ngdf[[\"BA_Pre_corr\", \"BA_Pre_pval\"]] = gdf.apply(lambda row: compute_correlation(row, pre_columns), axis=1, result_type='expand')\n\n# Save the results\ngdf.to_file(\"/your_path_here/grid5_Sta_corr.shp\")\n\nprint(\"Pearson correlation analysis between BA and Tem, as well as BA and Pre, has been completed (excluding zero values). Results saved.\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Temporal analysis for remote sensing data</span>"
    ]
  },
  {
    "objectID": "sec3.html",
    "href": "sec3.html",
    "title": "\n3  Spatial analysis for remote sensing data\n",
    "section": "",
    "text": "3.1 Spatial hotspot identification\nWe demonstrate a spatial analysis workflow using the 2024 climate and bushfire data as an example.\nWe will perform spatial hot spot and cold spot analysis using the rgeoda package and conduct spatial stratified heterogeneity analysis using the GD package.\nYou can install the required packages using the following commands in the R console:\nNow we read the data to R and perform some basic data exploration：\nSince the row and column numbers of the three rasters are not aligned, we first convert the non-NA cells of the temperature raster into a spatial polygon format. Then, we perform zonal statistics on temperature and wildfire data, and finally, remove all NA values corresponding to the three variables.\nsave the data to a shapefile:\nCodelibrary(sf)\nlibrary(rgeoda)\nlibrary(ggplot2)\n\nburnedarea = read_sf('./data/3. Spatial analysis/burnedarea_2024.shp')\nqueen_w = queen_weights(burnedarea)\nlisa = local_gstar(queen_w,  burnedarea[\"burnedarea\"])\ncats = lisa_clusters(lisa,cutoff = 0.05)\nburnedarea$hcp = factor(lisa_labels(lisa)[cats + 1],level = lisa_labels(lisa))\n\np_color = lisa_colors(lisa)\nnames(p_color) = lisa_labels(lisa)\np_label = lisa_labels(lisa)[sort(unique(cats + 1))]\nggplot(burnedarea) +\n  geom_sf(aes(fill = hcp)) +\n  scale_fill_manual(\n    values = p_color, \n    labels = p_label) +\n  theme_minimal() +\n  labs(fill = \"Cluster Type\")\n\n\n\n\n\n\nFigure 3.3: Bushfire burned area spatial hotspot analysis",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial analysis for remote sensing data</span>"
    ]
  },
  {
    "objectID": "sec3.html#geographical-detector-for-spatial-heterogeneity-and-factor-analysis",
    "href": "sec3.html#geographical-detector-for-spatial-heterogeneity-and-factor-analysis",
    "title": "\n3  Spatial analysis for remote sensing data\n",
    "section": "\n3.2 Geographical detector for spatial heterogeneity and factor analysis",
    "text": "3.2 Geographical detector for spatial heterogeneity and factor analysis\n\n\n\n\n\n\nAim\n\n\n\nThis step is designed to identify the climatic driving factors of bushfire burned area. We will use the GD package to analyze the power of determinant of climatic drivers on bushfire burned area based on the optimal parameter geographical detector model.\n\n\n\n\n\n\n\n\nDescription of steps\n\n\n\n\nLoad necessary libraries (sf for spatial data and gdverse for geographical detector analysis).\nRead the burned area and climate data.\nRun the OPGD model.\n\n\n\n\nCodelibrary(sf)\nlibrary(GD)\n\nburnedarea = read_sf('./data/3. Spatial analysis/burnedarea_2024.shp')\nopgd.m = gdm(burnedarea~tem+pre, \n             continuous_variable = c(\"tem\", \"pre\"),\n             data = sf::st_drop_geometry(burnedarea),\n             discmethod = c(\"equal\",\"natural\",\"quantile\"), \n             discitv = 3:15)\nopgd.m\n# Explanatory variables include 2 continuous variables.\n# \n# optimal discretization result of tem\n# method             :  quantile\n# number of intervals:  13\n# intervals:\n#  26.7 26.8 27 27.2 27.3 27.5 27.7 27.7 27.7 27.8 27.9 28 28 28.2\n# numbers of data within intervals:\n#  52 47 52 47 53 47 50 49 50 50 48 50 49\n# \n# optimal discretization result of pre\n# method             :  natural\n# number of intervals:  14\n# intervals:\n#  1589 1650 1712 1736 1758 1787 1816 1844 1870 1896 1924 1949 1977 2011 2067\n# numbers of data within intervals:\n#  3 16 71 40 40 44 50 76 68 62 57 52 43 22\n# \n# Geographical detectors results:\n# \n# Factor detector:\n#   variable     qv      sig\n# 1      tem 0.1301 1.79e-10\n# 2      pre 0.0733 2.86e-05\n# \n# Risk detector:\n# tem\n#              itv meanrisk\n# 1   [26.69,26.8]     3730\n# 2   (26.8,27.02]     4871\n# 3   (27.02,27.2]     5984\n# 4   (27.2,27.33]     5647\n# 5  (27.33,27.51]     5514\n# 6  (27.51,27.66]     9252\n# 7  (27.66,27.69]     8948\n# 8  (27.69,27.74]     6647\n# 9  (27.74,27.83]     9489\n# 10 (27.83,27.91]     7340\n# 11 (27.91,27.96]     5083\n# 12 (27.96,28.02]     7738\n# 13 (28.02,28.16]     7136\n# \n# pre\n#                    itv meanrisk\n# 1  [1.59e+03,1.65e+03]     3435\n# 2  (1.65e+03,1.71e+03]     5166\n# 3  (1.71e+03,1.74e+03]     4348\n# 4  (1.74e+03,1.76e+03]     4829\n# 5  (1.76e+03,1.79e+03]     7293\n# 6  (1.79e+03,1.82e+03]     5739\n# 7  (1.82e+03,1.84e+03]     6155\n# 8  (1.84e+03,1.87e+03]     7578\n# 9   (1.87e+03,1.9e+03]     7887\n# 10  (1.9e+03,1.92e+03]     7212\n# 11 (1.92e+03,1.95e+03]     7933\n# 12 (1.95e+03,1.98e+03]     8390\n# 13 (1.98e+03,2.01e+03]     6340\n# 14 (2.01e+03,2.07e+03]     7009\n# \n# tem\n#         interval [26.69,26.8] (26.8,27.02] (27.02,27.2]\n# 1   [26.69,26.8]         &lt;NA&gt;         &lt;NA&gt;         &lt;NA&gt;\n# 2   (26.8,27.02]            N         &lt;NA&gt;         &lt;NA&gt;\n# 3   (27.02,27.2]            Y            N         &lt;NA&gt;\n# 4   (27.2,27.33]            Y            N            N\n# 5  (27.33,27.51]            Y            N            N\n# 6  (27.51,27.66]            Y            Y            Y\n# 7  (27.66,27.69]            Y            Y            Y\n# 8  (27.69,27.74]            Y            Y            N\n# 9  (27.74,27.83]            Y            Y            Y\n# 10 (27.83,27.91]            Y            Y            N\n# 11 (27.91,27.96]            N            N            N\n# 12 (27.96,28.02]            Y            Y            N\n# 13 (28.02,28.16]            Y            Y            N\n#    (27.2,27.33] (27.33,27.51] (27.51,27.66] (27.66,27.69]\n# 1          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;\n# 2          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;\n# 3          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;\n# 4          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;\n# 5             N          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;\n# 6             Y             Y          &lt;NA&gt;          &lt;NA&gt;\n# 7             Y             Y             N          &lt;NA&gt;\n# 8             N             N             Y             Y\n# 9             Y             Y             N             N\n# 10            N             Y             Y             N\n# 11            N             N             Y             Y\n# 12            Y             Y             N             N\n# 13            N             N             Y             N\n#    (27.69,27.74] (27.74,27.83] (27.83,27.91] (27.91,27.96]\n# 1           &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;\n# 2           &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;\n# 3           &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;\n# 4           &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;\n# 5           &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;\n# 6           &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;\n# 7           &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;\n# 8           &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;\n# 9              Y          &lt;NA&gt;          &lt;NA&gt;          &lt;NA&gt;\n# 10             N             Y          &lt;NA&gt;          &lt;NA&gt;\n# 11             N             Y             Y          &lt;NA&gt;\n# 12             N             N             N             Y\n# 13             N             Y             N             N\n#    (27.96,28.02] (28.02,28.16]\n# 1           &lt;NA&gt;          &lt;NA&gt;\n# 2           &lt;NA&gt;          &lt;NA&gt;\n# 3           &lt;NA&gt;          &lt;NA&gt;\n# 4           &lt;NA&gt;          &lt;NA&gt;\n# 5           &lt;NA&gt;          &lt;NA&gt;\n# 6           &lt;NA&gt;          &lt;NA&gt;\n# 7           &lt;NA&gt;          &lt;NA&gt;\n# 8           &lt;NA&gt;          &lt;NA&gt;\n# 9           &lt;NA&gt;          &lt;NA&gt;\n# 10          &lt;NA&gt;          &lt;NA&gt;\n# 11          &lt;NA&gt;          &lt;NA&gt;\n# 12          &lt;NA&gt;          &lt;NA&gt;\n# 13             N          &lt;NA&gt;\n# \n# pre\n#               interval [1.59e+03,1.65e+03] (1.65e+03,1.71e+03]\n# 1  [1.59e+03,1.65e+03]                &lt;NA&gt;                &lt;NA&gt;\n# 2  (1.65e+03,1.71e+03]                   N                &lt;NA&gt;\n# 3  (1.71e+03,1.74e+03]                   N                   N\n# 4  (1.74e+03,1.76e+03]                   N                   N\n# 5  (1.76e+03,1.79e+03]                   Y                   Y\n# 6  (1.79e+03,1.82e+03]                   N                   N\n# 7  (1.82e+03,1.84e+03]                   N                   N\n# 8  (1.84e+03,1.87e+03]                   Y                   Y\n# 9   (1.87e+03,1.9e+03]                   Y                   Y\n# 10  (1.9e+03,1.92e+03]                   Y                   N\n# 11 (1.92e+03,1.95e+03]                   Y                   Y\n# 12 (1.95e+03,1.98e+03]                   Y                   Y\n# 13 (1.98e+03,2.01e+03]                   N                   N\n# 14 (2.01e+03,2.07e+03]                   N                   N\n#    (1.71e+03,1.74e+03] (1.74e+03,1.76e+03] (1.76e+03,1.79e+03]\n# 1                 &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n# 2                 &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n# 3                 &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n# 4                    N                &lt;NA&gt;                &lt;NA&gt;\n# 5                    Y                   Y                &lt;NA&gt;\n# 6                    N                   N                   N\n# 7                    Y                   N                   N\n# 8                    Y                   Y                   N\n# 9                    Y                   Y                   N\n# 10                   Y                   Y                   N\n# 11                   Y                   Y                   N\n# 12                   Y                   Y                   N\n# 13                   Y                   N                   N\n# 14                   Y                   N                   N\n#    (1.79e+03,1.82e+03] (1.82e+03,1.84e+03] (1.84e+03,1.87e+03]\n# 1                 &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n# 2                 &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n# 3                 &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n# 4                 &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n# 5                 &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n# 6                 &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n# 7                    N                &lt;NA&gt;                &lt;NA&gt;\n# 8                    N                   N                &lt;NA&gt;\n# 9                    Y                   N                   N\n# 10                   N                   N                   N\n# 11                   Y                   Y                   N\n# 12                   Y                   Y                   N\n# 13                   N                   N                   N\n# 14                   N                   N                   N\n#    (1.87e+03,1.9e+03] (1.9e+03,1.92e+03] (1.92e+03,1.95e+03]\n# 1                &lt;NA&gt;               &lt;NA&gt;                &lt;NA&gt;\n# 2                &lt;NA&gt;               &lt;NA&gt;                &lt;NA&gt;\n# 3                &lt;NA&gt;               &lt;NA&gt;                &lt;NA&gt;\n# 4                &lt;NA&gt;               &lt;NA&gt;                &lt;NA&gt;\n# 5                &lt;NA&gt;               &lt;NA&gt;                &lt;NA&gt;\n# 6                &lt;NA&gt;               &lt;NA&gt;                &lt;NA&gt;\n# 7                &lt;NA&gt;               &lt;NA&gt;                &lt;NA&gt;\n# 8                &lt;NA&gt;               &lt;NA&gt;                &lt;NA&gt;\n# 9                &lt;NA&gt;               &lt;NA&gt;                &lt;NA&gt;\n# 10                  N               &lt;NA&gt;                &lt;NA&gt;\n# 11                  N                  N                &lt;NA&gt;\n# 12                  N                  N                   N\n# 13                  N                  N                   N\n# 14                  N                  N                   N\n#    (1.95e+03,1.98e+03] (1.98e+03,2.01e+03] (2.01e+03,2.07e+03]\n# 1                 &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n# 2                 &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n# 3                 &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n# 4                 &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n# 5                 &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n# 6                 &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n# 7                 &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n# 8                 &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n# 9                 &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n# 10                &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n# 11                &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n# 12                &lt;NA&gt;                &lt;NA&gt;                &lt;NA&gt;\n# 13                   Y                &lt;NA&gt;                &lt;NA&gt;\n# 14                   N                   N                &lt;NA&gt;\n# \n# Interaction detector:\n#   variable   tem pre\n# 1      tem    NA  NA\n# 2      pre 0.321  NA\n# \n# Ecological detector:\n#   variable  tem  pre\n# 1      tem &lt;NA&gt; &lt;NA&gt;\n# 2      pre    Y &lt;NA&gt;",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial analysis for remote sensing data</span>"
    ]
  },
  {
    "objectID": "sec4.html",
    "href": "sec4.html",
    "title": "\n4  Geospatial artificial intelligence (GeoAI) for remote sensing data\n",
    "section": "",
    "text": "4.1 Background and Simple Examples of Machine Learning\nMachine learning (ML) methods have gained popularity in geospatial research due to their ability to model nonlinear relationships from high-dimensional data. One recent development is GPBoost, which integrates tree boosting with Gaussian processes to model both fixed effects and spatial (or other structured) random effects.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Geospatial artificial intelligence (GeoAI) for remote sensing data</span>"
    ]
  },
  {
    "objectID": "sec4.html#geoai-for-spatial-prediction-of-future-bushfire",
    "href": "sec4.html#geoai-for-spatial-prediction-of-future-bushfire",
    "title": "\n4  Geospatial artificial intelligence (GeoAI) for remote sensing data\n",
    "section": "\n4.2 GeoAI for spatial prediction of future bushfire",
    "text": "4.2 GeoAI for spatial prediction of future bushfire\n\n4.2.1 Future climate data collection\n\n\n\n\n\n\nAim\n\n\n\nWe separately collected future climate datasets of temperature and precipitation for the time period 2021–2040 from WorldClim. The previous 2024 data will be used for model training, followed by predicting bushfire burn areas for the time period 2021–2040.\n\n\n\n\n\n\n\n\nDescription of steps\n\n\n\n\nDownload WorldClim future climate datasets of temperature and precipitation for the time period 2021–2040.\nPreprocess the downloaded future climate data and organize it into vector format for subsequent predictions.\n\n\n\nThe data is downloaded from the WorldClim website and stored in the ./data/4. GeoAI Modeling/ directory, with period 2041-2060, GCM：ACCESS-CM2, scenarios:ssp585 (Links:https://www.worldclim.org/data/cmip6/cmip6_clim30s.html) .\n\n4.2.2 GeoAI modelling\n\n\n\n\n\n\nAim\n\n\n\nThe step uses the gpboost model to fit temperature and precipitation data in 2024 for the next step of spatial prediction.\n\n\n\n\n\n\n\n\nDescription of steps\n\n\n\n\nLoad necessary libraries and data.\nBuild the GPBoost model using the data processed in Section 3.\nYou can also explore traditional machine learning models using the caret package.\n\n\n\nSee here for more details about the gpboost model on spatio-temporal data.\n\nCodelibrary(sf)\n\nd24 = read_sf('./data/3. Spatial analysis/burnedarea_2024.shp')\nnames(d24) = c(\"pre\",\"tem\",\"burned\",\"geometry\")\nd24\n# Simple feature collection with 644 features and 3 fields\n# Geometry type: POLYGON\n# Dimension:     XY\n# Bounding box:  xmin: 129 ymin: -14.9 xmax: 131 ymax: -13.3\n# Geodetic CRS:  WGS 84\n# # A tibble: 644 × 4\n#     pre   tem burned                                        geometry\n#   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;                                   &lt;POLYGON [°]&gt;\n# 1  28.0 1893.   1250 ((130 -13.3, 130 -13.3, 130 -13.3, 130 -13.3, …\n# 2  28.0 1856.   1529 ((130 -13.3, 130 -13.3, 130 -13.3, 130 -13.3, …\n# 3  28.0 1888.   2203 ((130 -13.3, 130 -13.3, 130 -13.3, 130 -13.3, …\n# 4  28.0 1884.   1289 ((131 -13.3, 131 -13.3, 131 -13.3, 131 -13.3, …\n# 5  28.0 1865.   4247 ((131 -13.3, 131 -13.3, 131 -13.3, 131 -13.3, …\n# 6  28.0 1873.   1991 ((131 -13.3, 131 -13.3, 131 -13.3, 131 -13.3, …\n# # ℹ 638 more rows\n\nlibrary(gpboost)\ngp_model = GPModel(gp_coords = sdsfun::sf_coordinates(d24), \n                   cov_function = \"exponential\")\n# Training\nbst = gpboost(data = as.matrix(sf::st_drop_geometry(d24)[,1:2]), \n              label = as.matrix(sf::st_drop_geometry(d24)[,3,drop = FALSE]), \n              gp_model = gp_model, objective = \"regression_l2\", verbose = 0)\nbst\n# &lt;gpb.Booster&gt;\n#   Public:\n#     add_valid: function (data, name, valid_set_gp = NULL, use_gp_model_for_validation = TRUE) \n#     best_iter: -1\n#     best_score: NA\n#     current_iter: function () \n#     dump_model: function (num_iteration = NULL, feature_importance_type = 0L) \n#     eval: function (data, name, feval = NULL) \n#     eval_train: function (feval = NULL) \n#     eval_valid: function (feval = NULL) \n#     finalize: function () \n#     initialize: function (params = list(), train_set = NULL, modelfile = NULL, \n#     lower_bound: function () \n#     params: list\n#     predict: function (data, start_iteration = NULL, num_iteration = NULL, \n#     raw: NA\n#     record_evals: list\n#     reset_parameter: function (params, ...) \n#     rollback_one_iter: function () \n#     save: function () \n#     save_model: function (filename, start_iteration = NULL, num_iteration = NULL, \n#     save_model_to_string: function (start_iteration = NULL, num_iteration = NULL, feature_importance_type = 0L, \n#     set_train_data_name: function (name) \n#     to_predictor: function () \n#     update: function (train_set = NULL, fobj = NULL) \n#     upper_bound: function () \n#   Private:\n#     eval_names: NULL\n#     fixed_effect_train_loaded_from_file: NULL\n#     get_eval_info: function () \n#     gp_model: GPModel, R6\n#     gp_model_prediction_data_loaded_from_file: FALSE\n#     handle: gpb.Booster.handle\n#     has_gp_model: TRUE\n#     higher_better_inner_eval: NULL\n#     init_predictor: NULL\n#     inner_eval: function (data_name, data_idx, feval = NULL) \n#     inner_predict: function (idx) \n#     is_predicted_cur_iter: list\n#     label_loaded_from_file: NULL\n#     name_train_set: training\n#     name_valid_sets: list\n#     num_class: 1\n#     num_dataset: 1\n#     predict_buffer: list\n#     residual_loaded_from_file: NULL\n#     set_objective_to_none: FALSE\n#     train_set: gpb.Dataset, R6\n#     train_set_version: 1\n#     use_gp_model_for_validation: TRUE\n#     valid_sets: list\n#     valid_sets_gp: list\n\n\nExample: Machine learning using caret\n\nCodelibrary(caret)\n\n# Prepare non-spatial data\ndf = sf::st_drop_geometry(d24)\n\n# Train linear model with 5-fold CV\nctrl = trainControl(method = \"cv\", number = 5)\n\nmodel_caret = train(\n  burned ~ pre + tem, \n  data = df, \n  method = \"lm\", \n  trControl = ctrl\n)\n\nmodel_caret\n# Linear Regression \n# \n# 644 samples\n#   2 predictor\n# \n# No pre-processing\n# Resampling: Cross-Validated (5 fold) \n# Summary of sample sizes: 515, 515, 516, 516, 514 \n# Resampling results:\n# \n#   RMSE  Rsquared  MAE \n#   4653  0.0672    3927\n# \n# Tuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n4.2.3 Model validation\n\n\n\n\n\n\nWhy model validation?\n\n\n\nModel validation ensures that your model performs well on unseen data and helps avoid overfitting.\n\n\nCommon validation methods\n\nTrain/test split\nK-fold cross-validation\nLeave-one-out cross-validation (LOOCV)\nCommon evaluation metrics\n\n\nRMSE (Root Mean Squared Error)\n\n\nMAE (Mean Absolute Error)\n\n\nR² (Coefficient of determination)\nExample: Random forest model with RMSE\n\nCodeset.seed(123)\nmodel_rf = train(\n  burned ~ pre + tem, \n  data = df, \n  method = \"rf\", \n  trControl = ctrl,\n  metric = \"RMSE\"\n)\n# note: only 1 unique complexity parameters in default grid. Truncating the grid to 1 .\n\nmodel_rf$results\n#   mtry RMSE Rsquared  MAE RMSESD RsquaredSD MAESD\n# 1    2 4449    0.175 3545    193     0.0224   131\n\n\n\n4.2.4 Spatial prediction\n\n\n\n\n\n\nAim\n\n\n\nIn this step, the gpboost model constructed in the previous step is used to predict the burned area of 2030.\n\n\n\n\n\n\n\n\nDescription of steps\n\n\n\n\nRead the futural climate data.\nProcess the data to use the gpboost model to predict.\nDo spatial prediction by the gpboost model.\n\n\n\n\nCodelibrary(terra)\n\npref = terra::rast('./data/4. GeoAI Modeling/future_prec.tif')\npref\n# class       : SpatRaster \n# size        : 193, 154, 12  (nrow, ncol, nlyr)\n# resolution  : 0.00833, 0.00833  (x, y)\n# extent      : 129, 131, -14.9, -13.3  (xmin, xmax, ymin, ymax)\n# coord. ref. : lon/lat WGS 84 (EPSG:4326) \n# source      : future_prec.tif \n# names       : wc2.1~ec_01, wc2.1~ec_02, wc2.1~ec_03, wc2.1~ec_04, wc2.1~ec_05, wc2.1~ec_06, ...\npref = terra::app(pref,fun = \"sum\",na.rm = TRUE)\nnames(pref) = \"pre\"\npref\n# class       : SpatRaster \n# size        : 193, 154, 1  (nrow, ncol, nlyr)\n# resolution  : 0.00833, 0.00833  (x, y)\n# extent      : 129, 131, -14.9, -13.3  (xmin, xmax, ymin, ymax)\n# coord. ref. : lon/lat WGS 84 (EPSG:4326) \n# source(s)   : memory\n# name        :  pre \n# min value   : 1109 \n# max value   : 1750\n\ntemf = terra::rast('./data/4. GeoAI Modeling/future_tmax.tif')\ntemf\n# class       : SpatRaster \n# size        : 193, 154, 12  (nrow, ncol, nlyr)\n# resolution  : 0.00833, 0.00833  (x, y)\n# extent      : 129, 131, -14.9, -13.3  (xmin, xmax, ymin, ymax)\n# coord. ref. : lon/lat WGS 84 (EPSG:4326) \n# source      : future_tmax.tif \n# names       : wc2.1~ax_01, wc2.1~ax_02, wc2.1~ax_03, wc2.1~ax_04, wc2.1~ax_05, wc2.1~ax_06, ...\ntemf = terra::app(temf,fun = \"mean\",na.rm = TRUE)\nnames(temf) = \"tem\"\ntemf\n# class       : SpatRaster \n# size        : 193, 154, 1  (nrow, ncol, nlyr)\n# resolution  : 0.00833, 0.00833  (x, y)\n# extent      : 129, 131, -14.9, -13.3  (xmin, xmax, ymin, ymax)\n# coord. ref. : lon/lat WGS 84 (EPSG:4326) \n# source(s)   : memory\n# name        :  tem \n# min value   : 33.9 \n# max value   : 37.4\n\nd30 = c(pref,temf)\nd30 = d30 |&gt; \n  terra::as.polygons(aggregate = FALSE) |&gt; \n  sf::st_as_sf() |&gt; \n  dplyr::filter(dplyr::if_all(1:2,\\(.x) !is.na(.x)))\nd30\n# Simple feature collection with 16430 features and 2 fields\n# Geometry type: POLYGON\n# Dimension:     XY\n# Bounding box:  xmin: 129 ymin: -14.9 xmax: 131 ymax: -13.3\n# Geodetic CRS:  WGS 84\n# First 10 features:\n#     pre  tem                       geometry\n# 1  1739 34.6 POLYGON ((130 -13.3, 130 -1...\n# 2  1735 34.6 POLYGON ((130 -13.3, 130 -1...\n# 3  1745 34.6 POLYGON ((130 -13.3, 130 -1...\n# 4  1733 34.6 POLYGON ((130 -13.3, 130 -1...\n# 5  1732 34.6 POLYGON ((130 -13.3, 130 -1...\n# 6  1731 34.7 POLYGON ((130 -13.3, 130 -1...\n# 7  1725 34.8 POLYGON ((130 -13.3, 130 -1...\n# 8  1750 34.8 POLYGON ((130 -13.3, 130 -1...\n# 9  1741 34.8 POLYGON ((130 -13.3, 130 -1...\n# 10 1725 34.7 POLYGON ((130 -13.3, 130 -1...\n\npred = predict(bst, data = as.matrix(sf::st_drop_geometry(d30)[,1:2]), \n               gp_coords_pred = sdsfun::sf_coordinates(d30), \n               predict_var = TRUE, pred_latent = FALSE)\n\nd30$burned = pred$response_mean\nd30\n# Simple feature collection with 16430 features and 3 fields\n# Geometry type: POLYGON\n# Dimension:     XY\n# Bounding box:  xmin: 129 ymin: -14.9 xmax: 131 ymax: -13.3\n# Geodetic CRS:  WGS 84\n# First 10 features:\n#     pre  tem                       geometry burned\n# 1  1739 34.6 POLYGON ((130 -13.3, 130 -1...   3307\n# 2  1735 34.6 POLYGON ((130 -13.3, 130 -1...   3270\n# 3  1745 34.6 POLYGON ((130 -13.3, 130 -1...   3095\n# 4  1733 34.6 POLYGON ((130 -13.3, 130 -1...   3089\n# 5  1732 34.6 POLYGON ((130 -13.3, 130 -1...   3078\n# 6  1731 34.7 POLYGON ((130 -13.3, 130 -1...   3055\n# 7  1725 34.8 POLYGON ((130 -13.3, 130 -1...   3017\n# 8  1750 34.8 POLYGON ((130 -13.3, 130 -1...   2874\n# 9  1741 34.8 POLYGON ((130 -13.3, 130 -1...   2905\n# 10 1725 34.7 POLYGON ((130 -13.3, 130 -1...   2929",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Geospatial artificial intelligence (GeoAI) for remote sensing data</span>"
    ]
  },
  {
    "objectID": "sec1.html#aim",
    "href": "sec1.html#aim",
    "title": "\n1  Remote sensing data collection\n",
    "section": "\n1.3 🎯 Aim",
    "text": "1.3 🎯 Aim\nThis code is designed to compute and export the annual burned area from the MODIS MCD64A1 dataset using Google Earth Engine (GEE).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Remote sensing data collection</span>"
    ]
  },
  {
    "objectID": "sec2.html#temporal-trend-analysis-for-bushfire",
    "href": "sec2.html#temporal-trend-analysis-for-bushfire",
    "title": "2  Temporal analysis for remote sensing data",
    "section": "",
    "text": "Aim\n\n\n\nThis code is designed to compute the linear trend (slope and intercept) of burned area over time for each grid cell in a shapefile using linear regression, and save the results as a new shapefile.\n\n\n\n\n\n\n\n\nDescription of steps\n\n\n\n\nLoad necessary libraries (sf for spatial data and dplyr for data manipulation).\nRead the shapefile containing burned area data.\nExtract burned area column names corresponding to years 2000-2024.\nCreate a sequence of years as the independent variable for regression.\nPerform linear regression for each grid cell to estimate slope and intercept:\nUse lm() to fit a linear model with years as the independent variable.\nExtract the slope (trend of burned area change) and intercept.\nRemove the regression model objects to keep only numerical results.\nSave the updated shapefile with computed slope and intercept values.\n\n\n\n\n# Load necessary libraries\nlibrary(sf)\nlibrary(dplyr)\n\n# Read the shapefile\ngrid &lt;- st_read(\"/your_path_here/grid5_Sta.shp\")\n\n# Extract column names (ensure column order matches years)\nba_cols &lt;- paste0(\"BA_\", 2000:2024)  # Burned area for 25 years\n\n# Create a sequence of years (independent variable)\nyears &lt;- 2000:2024\n\n# Compute the regression slope and intercept for each grid cell\ngrid &lt;- grid %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    lm_model = list(lm(as.numeric(c_across(all_of(ba_cols))) ~ years)),  # Linear regression\n    slope = coef(lm_model)[2],   # Slope\n    intercept = coef(lm_model)[1]  # Intercept\n  ) %&gt;%\n  select(-lm_model) %&gt;%  # Remove model object\n  ungroup()\n\n# Save the new shapefile\nst_write(grid, \"/your_path_here/grid5_Linear.shp\", delete_layer = TRUE)\n\nimport geopandas as gpd\nimport numpy as np\nfrom scipy.stats import linregress\n\n# Read the Shapefile\ngdf = gpd.read_file(\"/your_path_here/grid5_Sta.shp\")\n\n# Extract column names for the time series\nyears = list(range(2000, 2025))  # 2000 to 2024\ncolumns = [f\"BA_{year}\" for year in years]\n\n# Ensure all columns exist in the dataset\ncolumns = [col for col in columns if col in gdf.columns]\n\n# Time axis\ntime = np.array(years[:len(columns)])\n\ndef compute_slope(row):\n    y_values = row[columns].values.astype(float)  # Retrieve the time series data for the pixel\n    if np.all(np.isnan(y_values)):  # If all values are NaN, return NaN\n        return np.nan\n    slope, _, _, _, _ = linregress(time, y_values)\n    return slope\n\n# Compute the slope and store it in a new column\ngdf[\"BA_slope\"] = gdf.apply(compute_slope, axis=1)\n\n# Save the results\ngdf.to_file(\"/your_path_here/grid5_Sta_slope.shp\")\n\nprint(\"Linear regression calculation completed, results saved.\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Temporal analysis for remote sensing data</span>"
    ]
  },
  {
    "objectID": "sec3.html#spatial-hotspot-identification",
    "href": "sec3.html#spatial-hotspot-identification",
    "title": "\n3  Spatial analysis for remote sensing data\n",
    "section": "",
    "text": "Aim\n\n\n\nThis step is designed to identify the spatial hot and cold spots of bushfire burned areas, which will be performed using the rgeoda package.\n\n\n\n\n\n\n\n\nDescription of steps\n\n\n\n\nLoad necessary libraries (sf for spatial data, rgeoda for spatial analysis, ggplot2 for data visualization).\nRead the burned area and climate data.\nCreate the spatial weight matrix.\nRun spatial hotspot analysis.\nPlot the result.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial analysis for remote sensing data</span>"
    ]
  },
  {
    "objectID": "sec4.html#analysing-future-bushfire-patterns",
    "href": "sec4.html#analysing-future-bushfire-patterns",
    "title": "\n4  Geospatial artificial intelligence (GeoAI) for remote sensing data\n",
    "section": "\n4.3 Analysing future bushfire patterns",
    "text": "4.3 Analysing future bushfire patterns\n\n\n\n\n\n\nAim\n\n\n\nIn this step, the predicted burned area of 2030 is used to analyse the future bushfire patterns.\n\n\n\n\n\n\n\n\nDescription of steps\n\n\n\n\nVisualise the predicted burned area of 2030.\nAnalyse the future bushfire patterns.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Geospatial artificial intelligence (GeoAI) for remote sensing data</span>"
    ]
  },
  {
    "objectID": "sec4.html#background-and-simple-examples-of-machine-learning",
    "href": "sec4.html#background-and-simple-examples-of-machine-learning",
    "title": "\n4  Geospatial artificial intelligence (GeoAI) for remote sensing data\n",
    "section": "",
    "text": "4.1.1 Example 1: Land Cover Classification from Remote Sensing Imagery\nSatellite images contain multispectral reflectance values across space. GPBoost can be used to classify land cover by combining decision tree boosting (e.g., LightGBM) with a Gaussian Process that accounts for spatial autocorrelation between nearby pixels.\n\n4.1.2 Example 2: Spatial Prediction of Vegetation Index\nNormalized Difference Vegetation Index (NDVI) values derived from remote sensing data often show spatial dependence. GPBoost allows prediction by combining covariates (e.g., elevation, temperature) with spatial random effects.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Geospatial artificial intelligence (GeoAI) for remote sensing data</span>"
    ]
  },
  {
    "objectID": "sec5.html",
    "href": "sec5.html",
    "title": "Practice and Example Datasets",
    "section": "",
    "text": "To facilitate student participation, we provide several example geospatial datasets that can be used directly (including temperature, precipitation, and wildfire burn area data for the corresponding regions in 2024), or participants may use their own data and study areas.\n\nExample Datasets\n\nBlue Mountains, New South Wales (Australia)\nA fire-prone mountainous region just west of Sydney, frequently affected by extreme bushfires such as the 2019–2020 Black Summer fires. Useful for studying peri-urban wildfire risk and climate influence.\ndata file\nAlpine Region, Victoria (Australia)\nLocated in the Victorian Alps northeast of Melbourne, this high-risk bushfire area experiences repeated large-scale wildfires due to dry summers and rugged terrain.\ndata file\nButte County, California (USA) Site of the 2018 Camp Fire, the deadliest wildfire in California history. Suitable for studying extreme wildfire impacts and recovery. data file\nShasta County, California (USA) Known for recurring large wildfires, including the 2018 Carr Fire. Features varied topography and dense vegetation. data file\nBoulder County, Colorado (USA) Located at the plains-mountain interface. High wildfire risk due to dry grasslands, suburban expansion, and strong wind events. data file\n\nParticipants are encouraged to explore these examples or bring their own datasets. GPBoost is especially effective in settings where structured covariates (e.g., reflectance bands, indices) and spatial dependence exist.",
    "crumbs": [
      "Practice and Example Datasets"
    ]
  },
  {
    "objectID": "sec6.html",
    "href": "sec6.html",
    "title": "Brief Introduction to Deep Learning and Transfer Learning",
    "section": "",
    "text": "What is Deep Learning?\nWhile GPBoost is primarily a structured machine learning method, deep learning has shown great promise in handling unstructured remote sensing data, such as imagery or time series.\nDeep learning models, such as Convolutional Neural Networks (CNNs), are commonly applied to:",
    "crumbs": [
      "Brief Introduction to Deep Learning and Transfer Learning"
    ]
  },
  {
    "objectID": "sec6.html#what-is-deep-learning",
    "href": "sec6.html#what-is-deep-learning",
    "title": "Brief Introduction to Deep Learning and Transfer Learning",
    "section": "",
    "text": "Image classification (e.g., identifying land use types)\nObject detection (e.g., buildings, roads)\nSemantic segmentation (e.g., per-pixel classification)",
    "crumbs": [
      "Brief Introduction to Deep Learning and Transfer Learning"
    ]
  },
  {
    "objectID": "sec6.html#what-is-transfer-learning",
    "href": "sec6.html#what-is-transfer-learning",
    "title": "Brief Introduction to Deep Learning and Transfer Learning",
    "section": "What is Transfer Learning?",
    "text": "What is Transfer Learning?\nTransfer learning involves fine-tuning a pre-trained model on a new task, which is useful when labeled data are limited. For instance:\n\nUsing a pre-trained ResNet model to classify crop types from drone imagery after fine-tuning on a small local dataset.",
    "crumbs": [
      "Brief Introduction to Deep Learning and Transfer Learning"
    ]
  },
  {
    "objectID": "sec6.html#complementarity-with-gpboost",
    "href": "sec6.html#complementarity-with-gpboost",
    "title": "Brief Introduction to Deep Learning and Transfer Learning",
    "section": "Complementarity with GPBoost",
    "text": "Complementarity with GPBoost\nIn certain workflows, deep learning can be used for feature extraction (e.g., producing features from images), which are then passed into GPBoost for structured modeling with spatial effects. This hybrid approach leverages both pixel-level detail and geostatistical structure.",
    "crumbs": [
      "Brief Introduction to Deep Learning and Transfer Learning"
    ]
  },
  {
    "objectID": "sec5.html#example-datasets",
    "href": "sec5.html#example-datasets",
    "title": "5  Student Challenge Example Datasets",
    "section": "",
    "text": "Sentinel-2 Land Cover Sample (China)\nA subset of Sentinel-2 imagery with land cover labels across several cities.\nMODIS NDVI Time-Series Data\nMonthly NDVI values from MODIS over East Asia, useful for modeling vegetation trends.\nSurface Temperature and Elevation Data (ASTER GDEM + MODIS LST)\nCombined raster datasets useful for spatial regression or GPBoost modeling.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Student Challenge Example Datasets</span>"
    ]
  }
]